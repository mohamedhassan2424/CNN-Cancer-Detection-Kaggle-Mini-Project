{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a5930e43e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#Load the modules\n",
    "from glob import glob \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras,cv2,os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "\n",
    "from tqdm import tqdm_notebook,trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc #garbage collection, we need to save all the RAM we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# Define data directories\n",
    "root_dir = \"../data/\"  # Modify this path for your local setup\n",
    "test_dir = root_dir + 'test/'\n",
    "train_dir = root_dir + 'train/'\n",
    "\n",
    "# Create DataFrame with image file paths\n",
    "image_df = pd.DataFrame({'image_path': glob(os.path.join(train_dir, '*.tif'))})  # Gather all TIFF files\n",
    "\n",
    "# Extract unique identifiers from file paths\n",
    "image_df['unique_id'] = image_df.image_path.map(lambda x: x.split('/')[3].split(\".\")[0])  # Parse file names for IDs\n",
    "\n",
    "# Import label information\n",
    "label_info = pd.read_csv(root_dir + \"train_labels.csv\")  # Load label data from CSV\n",
    "\n",
    "# Combine image paths with corresponding labels\n",
    "combined_df = image_df.merge(label_info, on=\"unique_id\")  # Join datasets based on unique ID\n",
    "\n",
    "# Display initial rows of the resulting DataFrame\n",
    "print(combined_df.head(3))  # Show first three entries for verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "def fetch_image_data(sample_size, dataframe):\n",
    "    \"\"\"\n",
    "    Retrieves a specified number of images and their corresponding labels from the given dataframe.\n",
    "    \"\"\"\n",
    "    # Initialize array for storing image data (sample_size, 96x96 pixels, RGB channels)\n",
    "    image_array = np.zeros([sample_size, 96, 96, 3], dtype=np.uint8)\n",
    "    \n",
    "    # Extract labels and convert to numpy array\n",
    "    label_array = np.squeeze(dataframe[['label']].values)[:sample_size]\n",
    "    \n",
    "    # Iterate through dataframe rows and load images\n",
    "    for index, entry in tqdm_notebook(dataframe.iterrows(), total=sample_size):\n",
    "        if index == sample_size:\n",
    "            break\n",
    "        image_array[index] = cv2.imread(entry['image_path'])\n",
    "    \n",
    "    return image_array, label_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import a subset of 10,000 image samples\n",
    "SAMPLE_SIZE = 10000\n",
    "image_data, labels = fetch_image_data(sample_size=SAMPLE_SIZE, dataframe=combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plot layout\n",
    "display_fig = plt.figure(figsize=(12, 5), dpi=120)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Select and display random images\n",
    "for plot_index, random_idx in enumerate(np.random.randint(0, SAMPLE_SIZE, 8)):\n",
    "    # Create subplot for each image\n",
    "    subplot = display_fig.add_subplot(2, 4, plot_index + 1, xticks=[], yticks=[])\n",
    "    \n",
    "    # Show image and its corresponding label\n",
    "    plt.imshow(image_data[random_idx])\n",
    "    subplot.set_title(f'Label: {labels[random_idx]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure for the bar chart\n",
    "chart_fig = plt.figure(figsize=(5, 3), dpi=120)\n",
    "\n",
    "# Calculate class distribution\n",
    "negative_count = (labels == 0).sum()\n",
    "positive_count = (labels == 1).sum()\n",
    "\n",
    "# Plot bar chart of class distribution\n",
    "plt.bar([0, 1], [positive_count, negative_count])\n",
    "\n",
    "# Customize x-axis labels\n",
    "plt.xticks([0, 1], [f\"Positive (N={positive_count})\", f\"Negative (N={negative_count})\"])\n",
    "\n",
    "# Add y-axis label\n",
    "plt.ylabel(\"# of samples\")\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Separate images based on their class labels\n",
    "class_1_images = image_data[labels == 1]\n",
    "class_0_images = image_data[labels == 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define number of histogram bins\n",
    "bin_count = 255\n",
    "\n",
    "# Create figure and subplots\n",
    "fig, axes = plt.subplots(4, 2, sharey=True, figsize=(10, 10), dpi=120)\n",
    "\n",
    "# Plot histograms for each color channel and combined RGB\n",
    "channels = ['Red', 'Green', 'Blue', 'RGB']\n",
    "for i, channel in enumerate(channels):\n",
    "    if channel == 'RGB':\n",
    "        axes[i, 0].hist(class_1_images.flatten(), bins=bin_count, density=True)\n",
    "        axes[i, 1].hist(class_0_images.flatten(), bins=bin_count, density=True)\n",
    "    else:\n",
    "        axes[i, 0].hist(class_1_images[:,:,:,i].flatten(), bins=bin_count, density=True)\n",
    "        axes[i, 1].hist(class_0_images[:,:,:,i].flatten(), bins=bin_count, density=True)\n",
    "    \n",
    "    axes[i, 1].set_ylabel(channel, rotation='horizontal', labelpad=35, fontsize=12)\n",
    "\n",
    "# Set titles and labels\n",
    "axes[0, 0].set_title(f\"Positive samples (N = {class_1_images.shape[0]})\")\n",
    "axes[0, 1].set_title(f\"Negative samples (N = {class_0_images.shape[0]})\")\n",
    "\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_ylabel(\"Relative Frequency\")\n",
    "\n",
    "axes[-1, 0].set_xlabel(\"Pixel Value\")\n",
    "axes[-1, 1].set_xlabel(\"Pixel Value\")\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set number of histogram bins for smoother visualization\n",
    "bin_count = 50\n",
    "\n",
    "# Create figure and subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, sharex=True, figsize=(10, 3), dpi=120)\n",
    "\n",
    "# Calculate and plot mean brightness histograms\n",
    "ax1.hist(np.mean(class_1_images, axis=(1,2,3)), bins=bin_count, density=True)\n",
    "ax2.hist(np.mean(class_0_images, axis=(1,2,3)), bins=bin_count, density=True)\n",
    "\n",
    "# Set titles and labels\n",
    "ax1.set_title(\"Average Intensity Distribution - Class 1\")\n",
    "ax2.set_title(\"Average Intensity Distribution - Class 0\")\n",
    "\n",
    "ax1.set_xlabel(\"Mean Image Intensity\")\n",
    "ax2.set_xlabel(\"Mean Image Intensity\")\n",
    "\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax2.set_ylabel(\"Density\")\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for histogram\n",
    "bin_count = 32  # Reduced number of bins for smoother distribution\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax_pos, ax_neg) = plt.subplots(1, 2, sharey=True, sharex=True, figsize=(10, 3), dpi=150)\n",
    "\n",
    "# Calculate and plot histogram for positive samples\n",
    "pos_brightness = np.mean(iceberg_samples, axis=(1,2,3))\n",
    "ax_pos.hist(pos_brightness, bins=bin_count, density=True)\n",
    "ax_pos.set_title(\"Mean brightness, positive samples\")\n",
    "ax_pos.set_xlabel(\"Image mean brightness\")\n",
    "ax_pos.set_ylabel(\"Relative frequenc\")\n",
    "\n",
    "# Calculate and plot histogram for negative samples\n",
    "neg_brightness = np.mean(ship_samples, axis=(1,2,3))\n",
    "ax_neg.hist(neg_brightness, bins=bin_count, density=True)\n",
    "ax_neg.set_title(\"Mean brightness, positive samples\")\n",
    "ax_neg.set_xlabel(\"Image mean brightness\")\n",
    "ax_neg.set_ylabel(\"Relative frequenc\")\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Determine the total number of images in the dataset\n",
    "total_images = combined_df[\"image_path\"].size\n",
    "\n",
    "# Load all images and their corresponding labels\n",
    "image_data, labels = fetch_image_data(sample_size=total_images, dataframe=combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "# Clear specific variables from memory\n",
    "class_1_images = None\n",
    "class_0_images = None\n",
    "\n",
    "# Trigger garbage collection to free up memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the proportion of data to use for training\n",
    "train_ratio = 0.75\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Generate shuffled indices\n",
    "shuffled_indices = np.random.permutation(labels.shape[0])\n",
    "\n",
    "# Apply shuffling to both image data and labels\n",
    "shuffled_images = image_data[shuffled_indices]\n",
    "shuffled_labels = labels[shuffled_indices]\n",
    "\n",
    "# Calculate the index to split the data\n",
    "split_point = int(np.round(train_ratio * shuffled_labels.shape[0]))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# (Note: we're not explicitly creating validation sets here, just preparing for the split)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Network architecture parameters\n",
    "conv_filter_size = (3,3)\n",
    "max_pool_size = (2,2)\n",
    "filters_layer1 = 128\n",
    "filters_layer2 = 64\n",
    "filters_layer3 = 32\n",
    "\n",
    "# Regularization parameters for preventing overfitting\n",
    "dropout_rate_conv = 0.3\n",
    "dropout_rate_dense = 0.5\n",
    "\n",
    "# Create the sequential model\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# Convolutional block 3\n",
    "cnn_model.add(Conv2D(filters_layer1, conv_filter_size, use_bias=False))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Activation(\"relu\"))\n",
    "cnn_model.add(Conv2D(filters_layer1, conv_filter_size, use_bias=False))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Activation(\"relu\"))\n",
    "cnn_model.add(MaxPool2D(pool_size = max_pool_size))\n",
    "cnn_model.add(Dropout(dropout_rate_conv))\n",
    "\n",
    "# Convolutional block 2\n",
    "cnn_model.add(Conv2D(filters_layer2, conv_filter_size, use_bias=False))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Activation(\"relu\"))\n",
    "cnn_model.add(Conv2D(filters_layer2, conv_filter_size, use_bias=False))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Activation(\"relu\"))\n",
    "cnn_model.add(MaxPool2D(pool_size = max_pool_size))\n",
    "cnn_model.add(Dropout(dropout_rate_conv))\n",
    "\n",
    "# Convolutional block 1\n",
    "cnn_model.add(Conv2D(filters_layer3, conv_filter_size, input_shape = (96, 96, 3)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Activation(\"relu\"))\n",
    "cnn_model.add(Conv2D(filters_layer3, conv_filter_size, use_bias=False))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Activation(\"relu\"))\n",
    "cnn_model.add(MaxPool2D(pool_size = max_pool_size)) \n",
    "cnn_model.add(Dropout(dropout_rate_conv))\n",
    "\n",
    "# Fully connected layer for feature extraction\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(256, use_bias=False))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Activation(\"relu\"))\n",
    "cnn_model.add(Dropout(dropout_rate_dense))\n",
    "\n",
    "# Output layer with sigmoid activation for binary classification\n",
    "cnn_model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the number of samples processed in each iteration\n",
    "samples_per_batch = 50\n",
    "\n",
    "# Configure the model for training\n",
    "cnn_model.compile(\n",
    "    # Use binary cross-entropy as the loss function for binary classification\n",
    "    loss=keras.losses.binary_crossentropy,\n",
    "    # Employ Adam optimizer with a specified learning rate\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    # Monitor accuracy during training\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training configuration\n",
    "num_epochs = 3\n",
    "batch_size = 50  # Assuming this was defined earlier\n",
    "\n",
    "# Iterate through epochs\n",
    "for epoch_num in range(num_epochs):\n",
    "    # Calculate number of batches per epoch\n",
    "    batch_count = np.floor(train_data_end / batch_size).astype(int)\n",
    "    \n",
    "    # Initialize metrics for this epoch\n",
    "    epoch_loss, epoch_accuracy = 0, 0\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    with trange(batch_count) as progress_bar:\n",
    "        for batch_idx in progress_bar:\n",
    "            # Determine batch boundaries\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "            \n",
    "            # Extract current batch\n",
    "            features_batch = X[batch_start:batch_end]\n",
    "            labels_batch = y[batch_start:batch_end]\n",
    "            \n",
    "            # Train model on current batch\n",
    "            batch_metrics = cnn_model.train_on_batch(features_batch, labels_batch)\n",
    "            \n",
    "            # Update running metrics\n",
    "            epoch_loss += batch_metrics[0]\n",
    "            epoch_accuracy += batch_metrics[1]\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_description(f'Running training epoch {epoch_num + 1}/{num_epochs}')\n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"{epoch_loss / (batch_idx + 1):.2f}\",\n",
    "                acc=f\"{epoch_accuracy / (batch_idx + 1):.2f}\"\n",
    "            )\n",
    "\n",
    "# Note: Data is not reshuffled between epochs due to in-place train/validation split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Calculate the number of batches needed for processing\n",
    "batch_count = np.floor((y.shape[0]-split_point) / samples_per_batch).astype(int) # Approximate calculation\n",
    "\n",
    "# Initialize running metrics\n",
    "running_loss, running_accuracy = 0, 0\n",
    "\n",
    "# Use trange for a progress bar during iteration\n",
    "with trange(batch_count) as progress_bar:\n",
    "    for batch_num in progress_bar:\n",
    "        # Determine the starting index for the current batch\n",
    "        batch_start = batch_num * samples_per_batch\n",
    "        \n",
    "        # Extract the current batch of data and labels\n",
    "        batch_data = X[batch_start:batch_start+samples_per_batch]\n",
    "        batch_labels = y[batch_start:batch_start+samples_per_batch]\n",
    "        \n",
    "        # Evaluate the model on the current batch\n",
    "        batch_metrics = cnn_model.test_on_batch(batch_data, batch_labels)\n",
    "        \n",
    "        # Update running metrics\n",
    "        running_loss += batch_metrics[0]\n",
    "        running_accuracy += batch_metrics[1]\n",
    "        \n",
    "        # Configure progress bar display\n",
    "        progress_bar.set_description('Running training')\n",
    "        progress_bar.set_postfix(\n",
    "            loss=\"%.2f\" % round(running_loss / (batch_num+1), 2),\n",
    "            acc=\"%.2f\" % round(running_accuracy / (batch_num+1), 2)\n",
    "        )\n",
    "\n",
    "# Print final validation results\n",
    "print(\"Validation loss:\", running_loss / batch_count)\n",
    "print(\"Validation accuracy:\", running_accuracy / batch_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clear data from memory\n",
    "feature_matrix = None\n",
    "target_vector = None\n",
    "\n",
    "# Invoke garbage collection to free up memory\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for test images\n",
    "test_image_directory = path + 'test/'\n",
    "\n",
    "# Get all .tif files in the test directory\n",
    "test_image_files = glob(os.path.join(test_image_directory, '*.tif'))\n",
    "\n",
    "# Initialize DataFrame for predictions\n",
    "prediction_results = pd.DataFrame()\n",
    "\n",
    "# Set batch size for processing\n",
    "images_per_batch = 5000\n",
    "\n",
    "# Get total number of test images\n",
    "total_images = len(test_image_files)\n",
    "\n",
    "# Process test images in batches\n",
    "for start_index in range(0, total_images, images_per_batch):\n",
    "    end_index = start_index + images_per_batch\n",
    "    print(f\"Processing images: {start_index} - {end_index}\")\n",
    "\n",
    "    # Create DataFrame for current batch\n",
    "    batch_df = pd.DataFrame({'image_path': test_image_files[start_index:end_index]})\n",
    "\n",
    "    # Extract image IDs from filenames\n",
    "    batch_df['image_id'] = batch_df.image_path.map(lambda x: x.split('/')[3].split(\".\")[0])\n",
    "\n",
    "    # Load images for current batch\n",
    "    batch_df['image_data'] = batch_df['image_path'].map(cv2.imread)\n",
    "\n",
    "    # Convert image data to numpy array\n",
    "    image_array = np.stack(batch_df[\"image_data\"].values)\n",
    "\n",
    "    # Generate predictions for current batch\n",
    "    batch_predictions = cnn_model.predict(image_array, verbose=1)\n",
    "\n",
    "    # Add predictions to DataFrame\n",
    "    batch_df['predicted_label'] = batch_predictions\n",
    "\n",
    "    # Append batch results to main results DataFrame\n",
    "    prediction_results = pd.concat([prediction_results, batch_df[[\"image_id\", \"predicted_label\"]]])\n",
    "\n",
    "# Display first few rows of results\n",
    "print(prediction_results.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Export prediction results to a CSV file\n",
    "prediction_results.to_csv(\"model_predictions.csv\", index=False, header=True)\n",
    "\n",
    "# The CSV file will contain the image IDs and their corresponding predicted labels\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
